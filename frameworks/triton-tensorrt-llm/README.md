# Triton Inference Server with TensorRT-LLM backend

## 1. Build the docker image
```bash
./build-image.sh
```

## 2. Build a Triton Inference Server model with a TensorRT-LLM engine
```bash
./build-image.sh
```

## 3. Run Triton Inference Server
```bash
./run.sh
```

## 4. Stop server
```bash
./stop.sh
```
